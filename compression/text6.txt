Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwcrrouffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).

Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).v
v
Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
v
v
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
vHuffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwhcBwos  c remna m yl  sand Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).v
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
x-special/nautilus-clipboard
copy
file:Zwrer ohome/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:Zwrer ohome/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.

Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
v
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
v
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).

7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special:wro ilus-clipboard
copy
file:///homeZwd r  hyam:wsuoiog ion1/testing/textfile.txt
x-special:wro ilus-clipboard
copy
file:///homeZwd r  hyam:wsuoiog ion1/testing/textfile.txt

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.

Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
v
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
v
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).

7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special:wro ilus-clipboard
copy
file:///homeZwd r  hyam:wsuoiog ion1/testing/textfile.txt
x-special:wro ilus-clipboard
copy
file:///homeZwd r  hyam:wsuoiog ion1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).

Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwcrrouffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wi whcBouffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
v
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwhhoToo 
r nr onautilus-clipboard
copy
fileZwrer o/home/ghanshyamGwnc tression1/testingZwsfoxdnerpxt
x-specialGw7wrrs -clipboard
copy
fileZwrer o/home/ghanshyamGwnc tression1/testingZwsfoxdnerpxt
Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.

Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwhcBwos  c remna m yl  sand Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1Zws ting/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1Zws ting/textfile.txt
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwcrrouffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).

Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).v
v
Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
v
v
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
vHuffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwhcBwos  c remna m yl  sand Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).v
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
x-special/nautilus-clipboard
copy
file:Zwrer ohome/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:Zwrer ohome/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.

Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
v
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
v
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).

7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special:wro ilus-clipboard
copy
file:///homeZwd r  hyam:wsuoiog ion1/testing/textfile.txt
x-special:wro ilus-clipboard
copy
file:///homeZwd r  hyam:wsuoiog ion1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).

Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwcrrouffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wi whcBouffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
v
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwhhoToo 
r nr onautilus-clipboard
copy
fileZwrer o/home/ghanshyamGwnc tression1/testingZwsfoxdnerpxt
x-specialGw7wrrs -clipboard
copy
fileZwrer o/home/ghanshyamGwnc tression1/testingZwsfoxdnerpxt
Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.

Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwhcBwos  c remna m yl  sand Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1Zws ting/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1Zws ting/textfile.txt
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwcrrouffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).

Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).v
v
Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
v
v
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
vHuffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwhcBwos  c remna m yl  sand Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).v
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
x-special/nautilus-clipboard
copy
file:Zwrer ohome/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:Zwrer ohome/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.

Huffman Encoding and Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
v
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
v
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).

7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
v
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
7wda nt by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwtolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. Bw,aking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPwor b  or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, PwsEG) are lossyâ€W
e
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. Fwnnnlahit video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87wig.su-nsdown to just 9we.shan klaiofa ours70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%woltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don9wehth  get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼
aa ahebta¼
argSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¥wustances each of 2wf0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and PwnhuAeo tp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. W
tolu doubt, follow the spec.
In the early 198wvp personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (wunny pictures of cats, videos, music and so on(woectto keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
Fwooo br rkletto use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPwor b  MPEG, or MPWwsare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zws, or pack
and programs like StuffIt or ZwsIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. BwsEG, MPEG(wiotlossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizws mi the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:wn xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and re8wrat  only 7wd  aat tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"%weltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€¼wvet any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 2wustances each of ¼
f0 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 9ws).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
109wr instructors, so it may not perfectly match the assignment this quarter. Wwsolu doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors(wntcompress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPwor b  MPEG) are lossyâ€%we
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 9wo.shan klaiofa ours70W
  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâwohth  get any special treatment; they
re8wraephe same 8 bits that are used for much rarer characters such as 'Ã¼waa ahebta2wargSh.le of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2wnsaeil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (wberfo r ealnsalways exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Wwph Schwarzwud Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this 8wia er. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10Pwr; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEGwsMPEG, or Pws3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Fwntapools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (wbsdvii esEG, MPwor iee are lossy9wehfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w  tthe original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'Ã¼wunTo ierhtt'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (wmrererec.ts); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97wo.
Huffman Encoding and W
0mnnc tression
Handout by Julie Zelenski with minor edits by DwnhoA brmicri and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106Bwustructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyGwond a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEGwsor MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyâwohfaAm uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70%w9woenyd Hnuah ace. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. Fwnxdnt 
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Fwoygiew
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ9whosget any special treatmentzwde cmrequire the same 8wi,s that are used for much rarer characters such as 'Ã¼' or 'Â¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits(wn no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 2¼
eil%wato  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
7ws  c remna m yl  sand Ww0mnnc tression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wlrt , personal computers had hard disks that were no larger than 10MBwn today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copyZwssh0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MPwrdt are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEGwsMPEG) are lossyâ€”decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to Ww t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donââwip get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼
y' or '¼
p'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8wsataemrps); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22weil”wu o  characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97(wwlul    c remna m yl  sand Dwiomnnc tression
Handout by Julie Gwuts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 19wmrt , personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy:wr h0xdnea ger a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as BwsEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEGwo are lossyâ9w  decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. Fwnoohh  data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inGwrei cstl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to %w t 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters donâ€™t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as '¼wy' or '¼wp'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits)zwntmore, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 22woil”e0aescharacters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Bw"e Zwnts ki with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about 7ws  c reoa m yl  sand about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MBzwnhr cp the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One techni8wopo use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, PwsEG, or MPW
sare specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (wom,as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or Gwo1oscan be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossyââwvhe
 uoiog ing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results inZwu xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 8ws characters down to ”w, 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"Wweltintelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (woemsl e or eight bits, where each
bit is either a 0 or a 1(wntstore each character. Common characters donâ€¼
vet any special treatment; they
require the same 8 bits that are used for much rarer characters such as '™wiieaa ahebta¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing ¼
ustances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special:wro ilus-clipboard
copy
file:///homeZwd r  hyam:wsuoiog ion1/testing/textfile.txt
x-special:wro ilus-clipboard
copy
file:///homeZwd r  hyam:wsuoiog ion1/testing/textfile.txt
