Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).

Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
vHuffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).v
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
Huffman Encoding and Data Compression
Handout by Julie Zelenski with minor edits by Keith Schwarz and Marty Stepp
This handout contains lots of supplemental background information about Huffman encoding and about file compression in
general. It should not be mandatory to read it, but you might find the information interesting, and it could help you to under -
stand the algorithm better to see more examples and discussion of it in this document. This handout was written by previous
106B instructors, so it may not perfectly match the assignment this quarter. When in doubt, follow the spec.
In the early 1980s, personal computers had hard disks that were no larger than 10MB; today, the puniest of
disks are still measured in hundreds of gigabytes. Even though hard drives are getting bigger, the files we
want to store (funny pictures of cats, videos, music and so on) seem to keep pace with that growth which
makes even today's gargantuan disk seem too small to hold everything.
One technique to use our storage more optimally is to compress the files. By taking advantage of redund -
ancy or patterns, we may be able to "abbreviate" the contents in such a way to take up less space yet main -
tain the ability to reconstruct a full version of the original when needed. Such compression could be useful
when trying to cram more things on a disk or to shorten the time needed to copy/send a file over a net -
work.
There are compression algorithms that you may already have heard of. Some compression formats, such
as JPEG, MPEG, or MP3, are specifically designed to handle a particular type of data file. They tend to
take advantage of known features of that type of data (such as the propensity for pixels in an image to be
same or similar colors to their neighbors) to compress it. Other tools such as compress, zip, or pack
and programs like StuffIt or ZipIt can be used to compress any sort of file. These algorithms have no
a priori expectations and usually rely on studying the particular data file contents to find redundancy and
patterns that allow for compression.
Some of the compression algorithms (e.g. JPEG, MPEG) are lossy—decompressing the compressed result
doesn't recreate a perfect copy of the original. Such an algorithm compresses by "summarizing" the data.
The summary retains the general structure while discarding the more minute details. For sound, video, and
images, this imprecision may be acceptable because the bulk of the data is maintained and a few missed
pixels or milliseconds of video delay is no big deal. For text data, though, a lossy algorithm usually isn't ap-
propriate. An example of a lossy algorithm for compressing text would be to remove all the vowels. Com-
pressing the previous sentence by this scheme results in:
n xmpl f lssy lgrthm fr cmprssng txt wld b t rmv ll th vwls.
This shrinks the original 87 characters down to just 61 and requires only 70% of the original space. To de -
compress, we could try matching the consonant patterns to English words with vowels inserted, but we
cannot reliably reconstruct the original in this manner. Is the compressed word "fr" an abbreviation for
the word "four" or the word "fir" or "far"? An intelligent reader can usually figure it out by context, but,
alas, a brainless computer can't be sure and would not be able to faithfully reproduce the original. For files
containing text, we usually want a lossless scheme so that there is no ambiguity when re-creating the original
meaning and intent.
An Overview
The standard ASCII character encoding uses the same amount of space (one byte or eight bits, where each
bit is either a 0 or a 1) to store each character. Common characters don’t get any special treatment; they
require the same 8 bits that are used for much rarer characters such as 'ü' or '¥'. A file of 1000 characters
encoded using the ASCII scheme will take 1000 bytes (8000 bits); no more, no less, whether it be a file of
1000 spaces to a file containing 4 instances each of 250 different characters. A fixed-length encoding like
ASCII is convenient because the boundaries between characters are easily determined and the pattern used
for each character is completely fixed (i.e. 'a' is always exactly 97).
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt
x-special/nautilus-clipboard
copy
file:///home/ghanshyam/compression1/testing/textfile.txt

